Although considerable research has been done on the brain networks specialized for visual processing of tools and bodies and the visual-motor processing of hand actions, these topics have largely been studied in isolation.
It remains unclear whether the highly specialized brain areas within these tool-, body-, and action-related networks in humans also play important roles in planning real movements with a tool or with the body (hand) alone.
These ROIs were defined by performing a whole-brain voxel-wise search contrasting the activity evoked during movement generation (i.e., movement planning [Plan phase] and execution [Execute phase]) vs the activity evoked during simple visual object presentation (Preview phase; when subjects had vision of the target object yet were unaware of which action [Grasp vs Reach] to perform).
First, predictive movement information, if it is to emerge, generally arises in the two time points prior to initiation of the movement (although note that in a few brain areas, such as L-pIPS and L-PMd, this information is also available prior to these two time points).
Indeed, although several other areas accurately differentiated the two upcoming movements for both effectors (e.g., post.
aIPS, aIPS, t-aIPS, and motor cortex), the preparatory spatial patterns of activity in these areas did not allow for accurate cross-classification.
Further investigation of the decoding accuracies within each area (using GG-corrected rmANOVAs and False Discovery Rate [FDR]-corrected follow-up paired sample t-tests) revealed only a few significant effects: in L-SPOC, decoding accuracies for the hand were significantly higher than for the tool and for across-effector decoding (both at p<0.05; F1.538 = 6.084, p=0.014); in L-SMG, decoding accuracies for the tool were significantly higher than for the hand and for across-effector decoding (both at p<0.05; F1.959 = 10.016, p<0.001); in L-motor cortex, decoding accuracies for the hand were significantly higher than across-effector decoding (p<0.05; F1.398 = 6.239, p=0.016); and lastly, in L-pMTG, decoding accuracies for the tool were significantly higher than for the hand (p=0.049; F1.968 = 4.171, p=0.037) (note that in L-EBA, although decoding accuracies for the hand showed a trend to be higher than for the tool, this did not reach significance; p=0.106; F1.370 = 3.635, p=0.078).
(Note that because our pattern classification analysis was performed on non-Talairached data [MVPA was in fact performed on single-subject ACPC-aligned data], comparing the weights across subjects on a single cortical surface was not feasible).
        Visual inspection of the voxel weightings failed to reveal any structured or consistent topography within or across subjects (for similar results, see also Harrison and Tong, 2009; Gallivan et al., 2011a).
We found that although accurate across-effector classification does indeed arise in Heschl’s gyrus during the trial, it does so distinctly earlier in the Plan-phase compared to that of the frontoparietal areas (Figure 4—figure supplement 3).

        
        It is worth emphasizing that while accurate decoding in a region points to underlying differences in the neural representations associated with different experimental conditions (e.g., for reviews see Haynes and Rees, 2006; Kriegeskorte, 2011; Naselaris et al., 2011; Norman et al., 2006), a lack of decoding or ‘null effect’ (i.e., 50% chance classification) can either reflect that the region 1) is not recruited for the conditions being compared, 2) contains neural/pattern differences between the conditions but which cannot be discriminated by the pattern classification algorithm employed (i.e., a limit of methodology, see Pereira et al., 2009; Pereira and Botvinick, 2011), or 3) is similarly (but non-discriminately) engaged in those conditions.
Although it is understandably difficult to rule out the second possibility (i.e., that voxel pattern differences exist but are not detected with the SVM classifiers), it is worth noting that we do in fact observe null-effects with the classifiers in several regions where they are to be expected.
Notably, however, in the majority of regions tested we find that neural representations remain linked to either the hand or tool.
The present findings provide insights into where different brain regions might be situated within such a hierarchy.
Although typically associated with visual-perceptual processing, EBA, like SPOC, has been implicated in coding movements of the hand/arm (Astafiev et al., 2004; Orlov et al., 2010, although see Peelen and Downing, 2005) and the fact that we were unable to decode tool movement plans from these regions suggests that they fail to incorporate tools into the body schema (see also Gallivan et al., 2009).
aIPS, aIPS, t-aIPS and motor cortex) that, although able to predict upcoming grasp vs reach movements with both the hand and the tool, did not generalize across the effector (i.e., no across-effector classification).
In addition to providing insights into how action-centred behavior is cortically represented (discussed above) these findings offer a new lens through which to view findings reported from previous observation-based fMRI studies.
When embedded within this larger context, however, it becomes important to not just understand how the actions of other individuals are represented but also how these perceptual representations may relate to the coding of self-generated motor actions.
Although others have shown that hand/arm movements can activate different regions in occipitotemporal cortex (Astafiev et al., 2004; Filimon et al., 2009; Cavina-Pratesi et al., 2010; Oosterhof et al., 2010; Orlov et al., 2010), here we demonstrate that these signals reflect the ‘intention’ to perform a motor act rather than the sensory feedback responses (visual, proprioceptive, tactile) that accompany it.
Given the delay of incoming sensory signals, this type of forward-state estimation is featured prominently in models of action control (Wolpert and Ghahramani, 2000; Wolpert and Flanagan, 2001) and, from the standpoint of perception, predicting the sensory consequences of movement can be used to disambiguate movements of the body (self) vs movements of the world (others) (von Helmohltz, 1866).
Updating the considerably simpler notion that action planning, particularly in the case of tool use, merely involves ‘access’ to ventral stream resources (Milner and Goodale, 1995; Valyear and Culham, 2010), these findings show that hand- and tool-related action plans can actually be decoded from preparatory signals in body- and tool-selective occipitotemporal cortex areas.
        In addition to suggesting a role for OTC in visual-motor planning, these findings might also shed light on the organizing principles of the ventral visual stream.
One particularly compelling alternative view, however, argues that the organization of OTC may be largely invariant to bottom-up visual properties and that it instead emerges as a by-product of the distinct connectivity patterns of OTC areas with the rest of the brain, particularly the downstream motor structures that use the visual information processed in OTC to plan movements of the body (Mahon et al., 2007; Mahon and Caramazza, 2009, 2011).
Where exactly the current findings fit within the context of these broader frameworks remains unclear, nevertheless, our results provide novel evidence suggesting that the specificity of visual object categorical responses in OTC are in some way linked to a specific role in preparing related motor behaviors.
      
Critically, however, this manipulation allowed us to maintain large retinal differences (i.e., position of the object with respect to fixation) and somatosensory differences (presence or absence of the tool in hand) between hand and tool runs.
Although including hand and tool trials within the same run would have enabled direct statistical comparisons between them, this would have necessitated insertion and removal of the tool during experimental testing, possibly leading to additional movement artifacts.
Although there were no visual differences between the Preview and Plan phase portions of the trial (i.e., the single object was always visually present), only in the Plan phase did participants have the necessary motor information in order to prepare the upcoming movement.
A testing session for each participant included set-up time (∼45 min), 8 functional runs (although two subjects participated in 6 and 10 functional runs, respectively) and 1 anatomical scan, and lasted approximately 3 hr.
More specifically, by training a pattern classifier to discriminate grasp vs reach movements with one effector (e.g., hand) and then testing whether that same classifier can be used to predict the same trial types with the other effector (e.g., tool), we could assess whether the object-directed action being planned (grasping vs reaching) was being represented with some level of invariance to the effector being used to perform the movement (see ‘Across-effector classification’ below for further details).
        
        
          
          MVPA was performed with a combination of in-house software (using Matlab) and the Princeton MVPA Toolbox for Matlab (http://code.google.com/p/princeton-mvpa-toolbox/) using a Support Vector Machines (SVM) binary classifier (libSVM, http://www.csie.ntu.edu.tw/~cjlin/libsvm/).
To control for the problem of multiple comparisons, a false discovery rate (FDR) correction of q ≤ 0.05 was applied based on all t-tests performed at each time point within an ROI (Benjamini and Yekutieli, 2001).
          Note that the data being used at any single time point (e.g., each TR in the time-resolved decoding approach) are independent as they are full trial-lengths removed from directly adjacent trials (recall that each trial = 34 s), providing more than adequate time for the hemodynamic responses associated with individual TRs used for classifier testing to sufficiently uncouple (this would not necessarily be the case in a rapid event-related design).
